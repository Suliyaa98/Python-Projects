# -*- coding: utf-8 -*-
"""DayLSTTSAXGBipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1phXh8O8u3A1fhBifjUDWFSuLXMNPCnoa
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import math
from statsmodels.graphics.tsaplots import plot_pacf
from matplotlib.offsetbox import AnchoredText
import xgboost as xgb
from sklearn.metrics import mean_squared_error

df = pd.read_csv('LST_day.csv')

df.head()

df.set_index('Date', inplace=True)

df.index = pd.to_datetime(df.index)

df

"""#1 Outlier Analysis and removal"""

# Removing null values
df.isnull().sum()

df = df.dropna()

df.isnull().sum()

# box and whisker plot
df.boxplot()

# remove outliers using iqr
Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1
df = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]

df.boxplot()

import pandas as pd

split_index = int(len(df) * 0.8)

# Get the datetime at the split index
split_date = df.index[split_index]

print("The split date for the 80:20 train-test split is:", split_date)

"""# Reviewing : Train/Test Split"""

train = df.loc[df.index < '2019-08-13']
test = df.loc[df.index >= '2019-08-13']

fig, ax = plt.subplots(figsize=(15, 5))
train.plot(ax=ax, label='Training Set', title='Data Train/Test Split')
test.plot(ax=ax, label='Test Set')
ax.axvline('2019-08-13', color='black', ls='--')
ax.legend(['Training Set', 'Test Set'])
plt.show()

"""# 1 Time series Cross Validation"""

from sklearn.model_selection import TimeSeriesSplit

tss = TimeSeriesSplit(n_splits=4, test_size=100)
df = df.sort_index()

df



fig, axs = plt.subplots(4, 1, figsize=(15, 15), sharex=True)

fold = 0
for train_idx, val_idx in tss.split(df):
    train = df.iloc[train_idx]
    test = df.iloc[val_idx]
    train['LST_Day'].plot(ax=axs[fold],
                          label='Training Set',
                          title=f'Data Train/Test Split Fold {fold}')
    test['LST_Day'].plot(ax=axs[fold],
                         label='Test Set')
    axs[fold].axvline(test.index.min(), color='black', ls='--')
    fold += 1
plt.show()

"""# 2. Forecasting Horizon Explained
The forecast horizon is the length of time into the future for which forecasts are to be prepared. These generally vary from short-term forecasting horizons (less than three months) to long-term horizons (more than two years).
"""

def create_features(df):
    """
    Create time series features based on time series index.
    """
    df = df.copy()
    df['dayofweek'] = df.index.dayofweek
    df['quarter'] = df.index.quarter
    df['month'] = df.index.month
    df['year'] = df.index.year
    df['dayofyear'] = df.index.dayofyear
    return df

df = create_features(df)

def plot_lag(prices, lag=1, ax=None, **kwargs):
    """
    Plot the relationship between coffee prices and their lagged values.

    Parameters:
    - prices: pd.Series
      The time series data of coffee prices.
    - lag: int
      The lag value to plot.
    - ax: matplotlib.axes._subplots.AxesSubplot, optional
      The axes to plot on.
    - kwargs: dict
      Additional keyword arguments for sns.regplot.

    Returns:
    - ax: matplotlib.axes._subplots.AxesSubplot
      The axes with the lag plot.
    """
    lagged_prices = prices.shift(lag)
    if ax is None:
        fig, ax = plt.subplots()
    scatter_kws = dict(alpha=0.75, s=3)
    line = dict(color='C3')

    ax = sns.regplot(x=lagged_prices, y=prices, scatter_kws=scatter_kws, line_kws=line, lowess=True, ax=ax, **kwargs)

    # Adding correlation on plot
    correlation_text = AnchoredText(f"{prices.corr(lagged_prices):.2f}", prop=dict(size="large"), frameon=True, loc="upper left")
    correlation_text.patch.set_boxstyle("square,pad=0.0")
    ax.add_artist(correlation_text)

    ax.set(title=f"Lag {lag} - LST_Night", xlabel='Lagged Prices', ylabel='LST')
    return ax

def plot_autocorrelation(prices, lags=6, lagplot_kwargs={}, **kwargs):
    """
    Plot the autocorrelation of coffee prices with their lagged values.

    Parameters:
    - prices: pd.Series
      The time series data of coffee prices.
    - lags: int
      The number of lags to plot.
    - lagplot_kwargs: dict, optional
      Additional keyword arguments for plot_lag.
    - kwargs: dict
      Additional keyword arguments for plt.subplots.

    Returns:
    - fig: matplotlib.figure.Figure
      The figure with the lag plots.
    """
    kwargs.setdefault("nrows", 2)
    kwargs.setdefault("ncols", math.ceil(lags / 2))
    kwargs.setdefault("figsize", (kwargs["ncols"] * 2, 2 * 2 + 0.5))
    fig, axs = plt.subplots(sharex=True, sharey=True, squeeze=False, **kwargs)

    for ax, k in zip(fig.get_axes(), range(kwargs["nrows"] * kwargs["ncols"])):
        if k + 1 <= lags:
            ax = plot_lag(prices, lag=k + 1, ax=ax, **lagplot_kwargs)
            ax.set_title(f"Lag #{k + 1}", fontdict=dict(fontsize=14))
            ax.set(xlabel="", ylabel="")
        else:
            ax.axis("off")

    plt.setp(axs[-1, :], xlabel='LST_day')
    fig.tight_layout(w_pad=0.1, h_pad=0.1)
    return fig

# Use the `df_avg_consumption` dataframe from the previous article.

_ = plot_autocorrelation(df["LST_Day"], lags=12)

_ = plot_pacf(df["LST_Day"], lags=12)

def create_lag_features(df, short_lags=7, weekly_lags=4, monthly_lags=3):
    """
    Create lag features for the coffee price time series data.

    Parameters:
    - df: pd.DataFrame
      The dataframe containing the coffee price data.
    - short_lags: int
      The number of short-term lags (e.g., 1 to 7 days).
    - weekly_lags: int
      The number of weekly lags (e.g., 7, 14, 21, 28 days).
    - monthly_lags: int
      The number of monthly lags (e.g., 30, 60, 90 days).

    Returns:
    - df: pd.DataFrame
      The dataframe with the added lag features.
    """
    y = df.loc[:, "LST_Day"]

    # Short-term lags (e.g., 1 to 7 days)
    for lag in range(1, short_lags + 1):
        df[f"lag_{lag}"] = y.shift(lag)

    # Weekly lags (e.g., 7, 14, 21, 28 days)
    for lag in range(1, weekly_lags + 1):
        df[f"lag_week_{lag}"] = y.shift(lag * 7)

    # Monthly lags (e.g., 30, 60, 90 days)
    for lag in range(1, monthly_lags + 1):
        df[f"lag_month_{lag}"] = y.shift(lag * 30)

    return df

# Apply the function to create a range of lags
df = create_lag_features(df, short_lags=7, weekly_lags=4, monthly_lags=3)

df

df.columns

tss = TimeSeriesSplit(n_splits=4, test_size=100)
df = df.sort_index()


fold = 0
preds = []
scores = []
for train_idx, val_idx in tss.split(df):
    train = df.iloc[train_idx]
    test = df.iloc[val_idx]

    train = (train)
    test = (test)

    FEATURES = [ 'dayofweek', 'quarter', 'month', 'year', 'dayofyear',
       'lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5', 'lag_6', 'lag_7',
       'lag_week_1', 'lag_week_2', 'lag_week_3', 'lag_week_4', 'lag_month_1',
       'lag_month_2', 'lag_month_3']
    TARGET = 'LST_Day'

    X_train = train[FEATURES]
    y_train = train[TARGET]

    X_test = test[FEATURES]
    y_test = test[TARGET]

from xgboost import XGBRegressor
import lightgbm as lgb
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV

# XGBoost
cv_split = TimeSeriesSplit(n_splits=4, test_size=100)
model = XGBRegressor()
parameters = {
    "max_depth": [1, 2, 3],
    "learning_rate": [0.01, 0.05, 0.1],
    "n_estimators": [100, 300, 500],
    "colsample_bytree": [0.5, 0.7, 1]
}


grid_search = GridSearchCV(estimator=model, cv=cv_split, param_grid=parameters)
grid_search.fit(X_train, y_train)

# Print the best parameters and score
print("Best Parameters:", grid_search.best_params_)
print("Best Score:", np.sqrt(-grid_search.best_score_))

"""# Train using Cross Validation"""

tss = TimeSeriesSplit(n_splits=4, test_size=100)
df = df.sort_index()


fold = 0
preds = []
scores = []
for train_idx, val_idx in tss.split(df):
    train = df.iloc[train_idx]
    test = df.iloc[val_idx]

    train = create_features(train)
    test = create_features(test)

    FEATURES = ['dayofweek', 'quarter', 'month', 'year', 'dayofyear',
       'lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5', 'lag_6', 'lag_7',
       'lag_week_1', 'lag_week_2', 'lag_week_3', 'lag_week_4', 'lag_month_1',
       'lag_month_2', 'lag_month_3']
    TARGET = 'LST_Night'

    X_train = train[FEATURES]
    y_train = train[TARGET]

    X_test = test[FEATURES]
    y_test = test[TARGET]

    reg = xgb.XGBRegressor(
        base_score=0.5,
        booster='gbtree',
        n_estimators=100,  # Reduced number of estimators
        objective='reg:squarederror',
        learning_rate=0.05,
        max_depth=2,
        colsample_bytree=0.5,
        reg_alpha=0.3,
        reg_lambda=4,
        early_stopping_rounds=100
    )
    reg.fit(X_train, y_train,
            eval_set=[(X_train, y_train), (X_test, y_test)],
            verbose=50)

    y_pred = reg.predict(X_test)
    preds.append(y_pred)
    score = np.sqrt(mean_squared_error(y_test, y_pred))
    scores.append(score)

xgb.plot_importance(reg)

print(f'Score across folds {np.mean(scores):0.4f}')
print(f'Fold scores:{scores}')

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

def calculate_metrics(y_true, y_pred):
    # Calculate RMSE (Root Mean Squared Error)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))

    # Calculate MAE (Mean Absolute Error)
    mae = mean_absolute_error(y_true, y_pred)

    # Calculate MAPE (Mean Absolute Percentage Error)
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100

    # Calculate R-squared (R2)
    r2 = r2_score(y_true, y_pred)

    return rmse, mae, mape, r2

rmse, mae, mape, r2 = calculate_metrics(y_test, y_pred)

print(f"RMSE: {rmse:.2f}")
print(f"MAE: {mae:.2f}")
print(f"MAPE: {mape:.2f}%")
print(f"R-squared: {r2:.2f}")

# Retraining on all data

df = create_features(df)
FEATURES = ['dayofweek', 'quarter', 'month', 'year', 'dayofyear',
       'lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5', 'lag_6', 'lag_7',
       'lag_week_1', 'lag_week_2', 'lag_week_3', 'lag_week_4', 'lag_month_1',
       'lag_month_2', 'lag_month_3']
TARGET = 'LST_Night'

X_all = df[FEATURES]
y_all = df[TARGET]

reg = xgb.XGBRegressor(
        base_score=0.5,
        booster='gbtree',
        n_estimators=100,  # Reduced number of estimators
        objective='reg:squarederror',
        learning_rate=0.05,
        max_depth=2,
        colsample_bytree=0.5,
        reg_alpha=0.3,
        reg_lambda=4)
reg.fit(X_all, y_all,
        eval_set=[(X_all, y_all)],
        verbose=100)

df.index.max()

df

# Create future dataframe
future = pd.date_range('2024-06-26 ','2025-06-26 ', freq='D')
future_df = pd.DataFrame(index=future)
future_df['isFuture'] = True
df['isFuture'] = False
df_and_future = pd.concat([df, future_df])
df_and_future = create_features(df_and_future)
df_and_future = create_lag_features(df_and_future, short_lags=7, weekly_lags=4, monthly_lags=3)

df_and_future

future_w_features = df_and_future.query('isFuture').copy()

future_w_features['pred'] = reg.predict(future_w_features[FEATURES])

# Plotting predictions
future_w_features['pred'].plot(figsize=(10, 5), ms=1, lw=1, title='Future Predictions')
plt.savefig('future_predictions.png', dpi = 800)
plt.show()

# Plot y_test and y_pred
plt.figure(figsize=(15, 6))
plt.plot(y_test.index, y_test, label='Actual')
plt.plot(y_test.index, y_pred, label='Predicted')
plt.title('Actual vs Predicted LST_NIGHT')
plt.xlabel('Date')
plt.ylabel('LST_Night')
plt.legend()
plt.show()

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(30, 13))

# Actual vs Predicted
axes[0].plot(y_test.index, y_test, label='Actual')
axes[0].plot(y_test.index, y_pred, label='Predicted')
axes[0].set_title('Actual vs Predicted LST_Night', fontsize=20, fontweight='bold')
axes[0].set_xlabel('Date', fontsize=15, fontweight='bold')
axes[0].set_ylabel('LST_NIGHT', fontsize=15, fontweight='bold')
axes[0].legend()

# Residual Plot
residuals = y_test - y_pred
sns.residplot(x=y_test, y=residuals, lowess=True, ax=axes[1], line_kws={'color': 'red', 'lw': 1})
axes[1].set_title('Residuals Plot', fontsize=20, fontweight='bold')
axes[1].set_xlabel('Observed Prices', fontsize=15, fontweight='bold')
axes[1].set_ylabel('Residuals', fontsize=15, fontweight='bold')

# Adding R2 value to the residual plot
r2_text = AnchoredText(f"R-squared = {r2:.2f}", loc="upper right", prop=dict(size=15))
axes[1].add_artist(r2_text)

# Making all tick labels bold
for ax in axes:
    for tick in ax.get_xticklabels() + ax.get_yticklabels():
        tick.set_fontweight('bold')
        tick.set_fontsize(15)  # Adjust the font size here as needed

plt.tight_layout()

plt.show()

