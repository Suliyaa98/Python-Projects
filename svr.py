# -*- coding: utf-8 -*-
"""SVR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QN2IL9UVY46GHS6mLALNoHYzAqO_7ZaE
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.svm import SVR
import warnings
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error
warnings.filterwarnings('ignore')

# Load the data
df = pd.read_csv('Final.csv')
df['Date'] = pd.to_datetime(df['Date'], format='%d-%m-%Y')

# Sort the DataFrame by 'Price Date' column
df = df.sort_values(by='Date')
df.set_index('Date', inplace=True)

df

# remove rows with outliers from the dataset on Price using IQR
Q1 = df['Price'].quantile(0.25)
Q3 = df['Price'].quantile(0.75)
IQR = Q3 - Q1
df = df[~((df['Price'] < (Q1 - 1.5 * IQR)) | (df['Price'] > (Q3 + 1.5 * IQR)))]

# scale the features
scaler = MinMaxScaler()
df[['Arrivals', 'Precipitation', 'Temp']] = scaler.fit_transform(df[['Arrivals', 'Precipitation', 'Temp']])

# Ensure the data is sorted by time
df = df.sort_index()


# Define features and target
FEATURES = ['Arrivals', 'YEAR', 'Dayofweek', 'Month', 'Precipitation', 'Temp',
       'Grade_encoded', 'Market Name_Kulasekaram',
       'Market Name_Namagiripettai', 'Market Name_Sathyamangalam',
       'Variety_Other', 'Variety_Poovan', 'Variety_Red Banana']

TARGET = 'Price'

# Split the data into train and test sets using train_test_split
train, test = train_test_split(df, test_size=0.2, shuffle=True, random_state=42)
X_train = train[FEATURES]
y_train = train[TARGET]

X_test = test[FEATURES]
y_test = test[TARGET]

# Select the first two features for visualization purposes
X_train_vis = X_train[['Arrivals', 'Precipitation']]
X_test_vis = X_test[['Arrivals', 'Precipitation']]

# Create SVR model
reg = SVR(kernel='rbf', gamma='scale', C=10, epsilon=1)
reg.fit(X_train_vis, y_train)

# Create mesh grid for plotting decision boundary
x1 = np.linspace(X_train_vis.iloc[:, 0].min(), X_train_vis.iloc[:, 0].max(), 100)
x2 = np.linspace(X_train_vis.iloc[:, 1].min(), X_train_vis.iloc[:, 1].max(), 100)
x1, x2 = np.meshgrid(x1, x2)
X_grid = np.c_[x1.ravel(), x2.ravel()]
y_grid = reg.predict(X_grid).reshape(x1.shape)

# Plot
plt.figure(figsize=(10, 6))
plt.scatter(X_train_vis.iloc[:, 0], y_train, color='green', label='Data points')
plt.scatter(X_train_vis.iloc[reg.support_, 0], y_train.iloc[reg.support_], facecolors='none', edgecolors='red', s=100, label='Support vectors')
plt.contour(x1, x2, y_grid, levels=[reg.epsilon], colors='blue', linestyles='--', label='Epsilon-tube')
plt.contour(x1, x2, y_grid, levels=[0], colors='orange', linestyles='-', label='Decision boundary')
plt.xlabel('First Feature (Arrivals)')
plt.ylabel('Price')
plt.title('SVR with RBF Kernel')
plt.legend()
plt.show()



#predict on test set
y_pred = reg.predict(X_test)

y_train_pred = reg.predict(X_train)

train_residuals = y_train - y_train_pred

test_residuals = y_test - y_pred

# Create a prediction error plot
fig, ax = plt.subplots(figsize=(8, 6))
ax.hist(train_residuals, bins=20, alpha=0.5, label='Training residuals')
ax.hist(test_residuals, bins=20, alpha=0.5, label='Test residuals')
ax.legend()
ax.axvline(0, color='k', linestyle='--', label='Zero Error')
ax.set_title('Prediction Error Plot')
ax.set_xlabel('Residuals')
ax.set_ylabel('Frequency')
plt.show()

# prompt: create a residual plot for the test set

import matplotlib.pyplot as plt
# Calculate the residuals for the test set
residuals = y_test - y_pred

# Create a residual plot for the test set
fig, ax = plt.subplots(figsize=(8, 6))
ax.scatter(x=range(len(residuals)), y=residuals, color='red', alpha=0.5)

# Set the plot title and labels
ax.set_title('Residual Plot for the Test Set')
ax.set_xlabel('Observation')
ax.set_ylabel('Residual')

# Add a horizontal line at zero
ax.axhline(y=0, color='r', linestyle='--')

# Show the plot
plt.show()

"""# Evaluation on train set"""

# evaluate on test set for rmse, r2, mae, mape
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error
print('RMSE:', np.sqrt(mean_squared_error(y_train, y_train_pred)))
print('R2:', r2_score(y_train, y_train_pred))
print('MAE:', mean_absolute_error(y_train, y_train_pred))
print('MAPE:', mean_absolute_percentage_error(y_train, y_train_pred)*100)



"""# evaluation on test set"""

# evaluate on test set for rmse, r2, mae, mape
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error
print('RMSE:', np.sqrt(mean_squared_error(y_test, y_pred)))
print('R2:', r2_score(y_test, y_pred))
print('MAE:', mean_absolute_error(y_test, y_pred))
print('MAPE:', mean_absolute_percentage_error(y_test, y_pred)*100)

# calcuate adjusted r2 on the test set

adj_r2 = 1 - (1-r2_score(y_test, y_pred))*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)
print('Adjusted R2:', adj_r2)

# Sort y_test and y_pred based on the index
y_test_sorted = y_test.sort_index()
y_pred_sorted = y_pred[np.argsort(y_test.index)]

# Create a figure with a single subplot
fig, ax = plt.subplots(figsize=(10, 6))

# Plot the actual test data in blue
ax.plot(y_test_sorted.index, y_test_sorted, color='black', label='Actual')

# Plot the predicted values in orange
ax.plot(y_test_sorted.index, y_pred_sorted, color='yellow', label='Predicted')

# Set the title and axis labels
ax.set_title('Model Predictions vs. Actual Test Data')
ax.set_xlabel('Date')
ax.set_ylabel('Price')

# Add a legend and show the plot
ax.legend()
plt.show()

# prompt: # draw the r2 plot

import matplotlib.pyplot as plt
# Calculate the R^2 value
r_squared = r2_score(y_test, y_pred)

# Create a scatter plot with predicted values on the x-axis and actual values on the y-axis
plt.scatter(y_pred, y_test)

# Add a line for the perfect fit (y = x)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red')

# Set the title and axis labels
plt.title('R^2 Plot')
plt.xlabel('Predicted Values')
plt.ylabel('Actual Values')

# Add the R^2 value to the plot
plt.text(0.5, 0.5, f'R^2 = {r_squared:.4f}', fontsize=16)

# Show the plot
plt.show()



"""# Perform Recursive Feature Elimination (RFE) with different number of features
rmse_scores = []
num_features_list = range(1, X.shape[1] + 1)

for num_features in num_features_list:
    rfe = RFE(estimator=model, n_features_to_select=num_features)
    rfe.fit(X_train, y_train)

    X_train_rfe = rfe.transform(X_train)
    X_test_rfe = rfe.transform(X_test)

    model.fit(X_train_rfe, y_train)
    y_pred = model.predict(X_test_rfe)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    rmse_scores.append(rmse)
"""

# prompt: make a dataframe svr with y_test and y_pred

import pandas as pd
svr = pd.DataFrame({'y_test': y_test_sorted, 'y_pred': y_pred_sorted})
svr

svr.to_csv('svr.csv', index=True)

